{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch + cuda: 2.7.1+cu126\n",
      "Is cuda avialable: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch + cuda:\", torch.__version__) # Check PyTorch version\n",
    "print(\"Is cuda avialable:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auth Token Setting:\n",
    "\n",
    "- HugginigFace Token\n",
    "- OpenAI Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install flash-attention protobuf einops pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms_info = {\n",
    "    \"Gemma-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"google/gemma-7b-it\",\n",
    "        \"model_path\": \"./LLMs/Gemma-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Gemma-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Mistral-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"model_path\": \"./LLMs/Mistral-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Mistral-7B-Instruct\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \"Phi3-Small-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"microsoft/Phi-3-small-128k-instruct\",\n",
    "        \"model_path\": \"./LLMs/Phi3-Small-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Phi3-Small-7B-Instruct\",\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\",\n",
    "            \"attn_implementation\": \"flash_attention_2\"\n",
    "        }\n",
    "    },\n",
    "    \"Qwen2-7B-Instruct\": {\n",
    "        \"remote_model_name\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Qwen2-7B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Qwen2-7B-Instruct\",\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "\n",
    "    \n",
    "    \"Llama3-1-8B\": {# Size: - , system RAM: 20 GB (Windows)\n",
    "        \"remote_model_name\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "        \"model_path\": \"./LLMs/Llama3-1-8B\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Llama3-1-8B\",\n",
    "        \"additional_config\": {\n",
    "            \n",
    "           \"rope_scaling\": {\n",
    "            \"type\": \"linear\",\n",
    "            \"factor\": 8.0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Llama3-1-8B-Instruct\": {# Size: - , system RAM: 20 GB (Windows)\n",
    "        \"remote_model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        \"model_path\": \"./LLMs/Llama3-1-8B-Instruct\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Llama3-1-8B-Instruct\",\n",
    "        \"additional_config\": {\n",
    "            \n",
    "           \"rope_scaling\": {\n",
    "            \"type\": \"linear\",\n",
    "            \"factor\": 8.0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"Vicuna-33B\": {\n",
    "        \"remote_model_name\": \"lmsys/vicuna-33b-v1.3\",\n",
    "        \"model_path\": \"./LLMs/Vicuna-33B\",\n",
    "        \"tokenizer_path\": \"./Tokenizers/Vicuna-33B\",\n",
    "        \"hf_token\": hf_token,\n",
    "        \"additional_config\": {\n",
    "            \"torch_dtype\": \"auto\",\n",
    "            \"device\": \"auto\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_key):\n",
    "    model_info = llms_info[model_key]\n",
    "    # Use .get() to provide a default empty dictionary if \"additional_config\" is not present\n",
    "    config = model_info[\"additional_config\"]\n",
    "\n",
    "    # Check if the directories for the model and tokenizer exist\n",
    "    model_dir_exists = os.path.isdir(model_info[\"model_path\"])\n",
    "    tokenizer_dir_exists = os.path.isdir(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    if model_dir_exists and tokenizer_dir_exists:\n",
    "        print(f\"{model_key} model and tokenizer are already present.\")\n",
    "    else:\n",
    "        print(f\"Downloading and saving model and tokenizer for {model_key}.\")\n",
    "        # Include the token in the download process if applicable\n",
    "        hf_token = model_info.get(\"hf_token\", None)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"model_path\"],\n",
    "            torch_dtype=getattr(torch, config.get(\"torch_dtype\", \"auto\")) if config.get(\"torch_dtype\", \"auto\") != \"auto\" else None,\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_info[\"remote_model_name\"],\n",
    "            cache_dir=model_info[\"tokenizer_path\"],\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        # Ensure directories are created during download\n",
    "        if not model_dir_exists:\n",
    "            os.makedirs(model_info[\"model_path\"], exist_ok=True)\n",
    "        if not tokenizer_dir_exists:\n",
    "            os.makedirs(model_info[\"tokenizer_path\"], exist_ok=True)\n",
    "        # # Save them locally\n",
    "        model.save_pretrained(model_info[\"model_path\"])\n",
    "        tokenizer.save_pretrained(model_info[\"tokenizer_path\"])\n",
    "\n",
    "    # Load model and tokenizer from local storage\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_info[\"model_path\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_info[\"tokenizer_path\"])\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model(\"Gemma-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = \"FoCus\"             # Synthetic-PersonaChat, Blended Skill Talk, PEC, ConvAI2, FoCus, IT-ConvAI2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas        0\n",
      "context         0\n",
      "act_response    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to visit the Nazareth House again...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>User2: The history of the house you are intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have been to Vermont a few times to go skiin...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: This house was use as a stop for slaves...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I would like to visit the Nazareth House again...   \n",
       "1  I have been to Vermont a few times to go skiin...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I think Ive been there before but I don...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: The history of the house you are intere...  \n",
       "1  User2: This house was use as a stop for slaves...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the data frame\n",
    "df = pd.read_csv(f'./datasets/{Dataset}/ds_cleaned.csv')\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Save the prompts\n",
    "df.to_csv('./Prompts/' + Dataset + '.csv', index=False)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>context</th>\n",
       "      <th>act_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to visit the Nazareth House again...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>User2: The history of the house you are intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have been to Vermont a few times to go skiin...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: This house was use as a stop for slaves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am fascinated by the Spanish Colonial Reviva...</td>\n",
       "      <td>User1: Wow, this is amazing! What is this?\\nUs...</td>\n",
       "      <td>User2: Sure, you will like to know that this p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to become a college student.I want to s...</td>\n",
       "      <td>User1: Where is this place?\\nUser2: Hello! Wel...</td>\n",
       "      <td>User2: Technische Universität Darmstadt in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like to visit england.I love church.I would ...</td>\n",
       "      <td>User1: Where is this place?\\nUser2: This place...</td>\n",
       "      <td>User2: I suggest a place, for your wish of see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I would like to go to University.I live in Mic...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>User2: They offer 132 bachelors degree program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love nice hotels.I would like to go to Calif...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>User2: Its current owner is Anbang Insurance G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I have the fantasy about valley.I like lakes.I...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: You can view Hat Creek valley and the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I am willing to start a seminary in New Brunsw...</td>\n",
       "      <td>User1: I think Ive been there before but I don...</td>\n",
       "      <td>User2: It was closely connected with Rutgers U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I like the National War Memorial.I hope to mov...</td>\n",
       "      <td>User1: I know this place, but I dont remember ...</td>\n",
       "      <td>User2: You have interest in history and will l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  I would like to visit the Nazareth House again...   \n",
       "1  I have been to Vermont a few times to go skiin...   \n",
       "2  I am fascinated by the Spanish Colonial Reviva...   \n",
       "3  I want to become a college student.I want to s...   \n",
       "4  I like to visit england.I love church.I would ...   \n",
       "5  I would like to go to University.I live in Mic...   \n",
       "6  I love nice hotels.I would like to go to Calif...   \n",
       "7  I have the fantasy about valley.I like lakes.I...   \n",
       "8  I am willing to start a seminary in New Brunsw...   \n",
       "9  I like the National War Memorial.I hope to mov...   \n",
       "\n",
       "                                             context  \\\n",
       "0  User1: I think Ive been there before but I don...   \n",
       "1  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "2  User1: Wow, this is amazing! What is this?\\nUs...   \n",
       "3  User1: Where is this place?\\nUser2: Hello! Wel...   \n",
       "4  User1: Where is this place?\\nUser2: This place...   \n",
       "5  User1: I think Ive been there before but I don...   \n",
       "6  User1: I think Ive been there before but I don...   \n",
       "7  User1: I know this place, but I dont remember ...   \n",
       "8  User1: I think Ive been there before but I don...   \n",
       "9  User1: I know this place, but I dont remember ...   \n",
       "\n",
       "                                        act_response  \n",
       "0  User2: The history of the house you are intere...  \n",
       "1  User2: This house was use as a stop for slaves...  \n",
       "2  User2: Sure, you will like to know that this p...  \n",
       "3  User2: Technische Universität Darmstadt in the...  \n",
       "4  User2: I suggest a place, for your wish of see...  \n",
       "5  User2: They offer 132 bachelors degree program...  \n",
       "6  User2: Its current owner is Anbang Insurance G...  \n",
       "7  User2: You can view Hat Creek valley and the T...  \n",
       "8  User2: It was closely connected with Rutgers U...  \n",
       "9  User2: You have interest in history and will l...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Loading the prompt\n",
    "df = pd.read_csv(f'./Prompts/{Dataset}.csv')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a prompt\n",
    "def create_prompt(personas, context, include_cot=False):\n",
    "\n",
    "    prompt = (\n",
    "        \"I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\\n\"\n",
    "        \"As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\\n\\n\"\n",
    "        \"Participant Personas:\\n\"\n",
    "        f\"{personas}\\n\\n\"\n",
    "        \"Conversation Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Task Instruction:\\n\"\n",
    "        \"* Provide an unannotated response.\\n\"\n",
    "        \"* If only one persona is available, personalize the response accordingly.\\n\"\n",
    "        \"* If the conversation context is a single query, respond appropriately to the query.\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            \"* Apply Chain of Thought reasoning to reflect on the alignment of your response with the personas.\\n\"\n",
    "        )\n",
    "\n",
    "    prompt += (\n",
    "        \"\\nOutput Format: only give a JSON of the following format:\\n\"\n",
    "        \"{\\n\"\n",
    "    )\n",
    "    \n",
    "    if include_cot:\n",
    "        prompt += (\n",
    "            '  \"reasoning\": \"briefly describe your personalization process (in 110 words or less).\"\\n'\n",
    "        )\n",
    "        \n",
    "    prompt += (\n",
    "        '  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\\n'\n",
    "        \"}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-Source LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma-7B-Instruct model and tokenizer are already present.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a2e21b9a744c5c827035a56e7a6194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LLM_name = \"Gemma-7B-Instruct\"                  # Mistral-7B-Instruct, Llama3-1-8B-Instruct, Qwen2-7B-Instruct, Gemma-7B-Instruct\n",
    "\n",
    "COT_SETUP = False                                  # Chain Of Thoughts Configuration\n",
    "\n",
    "# Assuming load_model is defined and works as expected\n",
    "model, tokenizer = load_model(LLM_name)\n",
    "\n",
    "\n",
    "MAX_NEW_TOKEN = 220 if COT_SETUP else 110\n",
    "\n",
    "generation_params = {\n",
    "    \n",
    "    \"max_new_tokens\": MAX_NEW_TOKEN,      # Based on max response length + reasoning\n",
    "    \"temperature\": 0,                     # Based on FELM paper (Greedy Setup)\n",
    "    \"do_sample\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will provide you with a conversation context and the personas of the participants, that can be annotated with speaker information.\n",
      "As a participant in this conversation, your task is to generate a personalized response, considering the conversation context and personas.\n",
      "\n",
      "Participant Personas:\n",
      "I like football.I have not been to South Africa.I am interested in the World Cup.I hope to become an architect.I would like to attend a rugby match.\n",
      "\n",
      "Conversation Context:\n",
      "User1: Where is this place?\n",
      "User2: The Mbombela Stadium is located in South Africa; a country you have yet to visit.\n",
      "User1: Why was the stadium built?\n",
      "User2: The stadium was built to host matches from the 2010 FIFA World Cup. This may excite you since you are interested in the world cup.\n",
      "User1: Are any other sports played here?\n",
      "User2: Yes the stadium is also used to host rugby matches. Maybe you should attend one here since I know you would like to attend a rugby match.\n",
      "User1: What is the capacity of the stadium?\n",
      "User2: The stadium has a capacity of 40,929 seats.\n",
      "User1: How long did it take to build the stadium?\n",
      "User2: It took two years to build. Construction started in 2007 and was completed in 2009.\n",
      "User1: Is there anything unique about the stadium design?\n",
      "\n",
      "Task Instruction:\n",
      "* Provide an unannotated response.\n",
      "* If only one persona is available, personalize the response accordingly.\n",
      "* If the conversation context is a single query, respond appropriately to the query.\n",
      "\n",
      "Output Format: only give a JSON of the following format:\n",
      "{\n",
      "  \"response\": \"provide the personalized natural language response here (in 110 words or less).\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_row = df.iloc[120]\n",
    "personas = first_row['personas']\n",
    "context = first_row['context']\n",
    "\n",
    "# Example usage\n",
    "prompt = create_prompt(personas, context, include_cot=COT_SETUP)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Generate a response\n",
    "output = generator(prompt, **generation_params)\n",
    "response = output[0]['generated_text'][len(prompt):]\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1183/1183 [1:42:45<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Iterate through the DataFrame and generate responses\n",
    "gen_responses = []\n",
    "response_times = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating responses\"):\n",
    "    personas = row['personas']\n",
    "    context = row['context']\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(personas, context, COT_SETUP)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    output = generator(prompt, **generation_params)[0]['generated_text']\n",
    "    \n",
    "    # Measure the end time and calculate the duration\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "    \n",
    "    response = output[len(prompt):]\n",
    "\n",
    "    # Store the generated response and response time\n",
    "    gen_responses.append(response)\n",
    "    response_times.append(response_time)\n",
    "\n",
    "# Create a DataFrame with the responses and response times\n",
    "response_df = pd.DataFrame({\n",
    "    'gen_response': gen_responses,\n",
    "    'response_time': response_times\n",
    "})  \n",
    "\n",
    "COT_ = \"-COT\" if COT_SETUP else \"\" \n",
    "\n",
    "# Save the response DataFrame to a CSV and Excel file\n",
    "response_df.to_csv(f'./Raw Responses/{Dataset}/{LLM_name}{COT_}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.293113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.285029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.283818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.083229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.305366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>4.180840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.327561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.306831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.300904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.296530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.282367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.311118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>4.212578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.330694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.317040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.317598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.314490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.316728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.322318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...</td>\n",
       "      <td>5.323620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         gen_response  response_time\n",
       "0   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.293113\n",
       "1   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.285029\n",
       "2   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.283818\n",
       "3   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.083229\n",
       "4   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.305366\n",
       "5   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       4.180840\n",
       "6   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.327561\n",
       "7   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.306831\n",
       "8   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.300904\n",
       "9   ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.296530\n",
       "10  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.282367\n",
       "11  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.311118\n",
       "12  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       4.212578\n",
       "13  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.330694\n",
       "14  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.317040\n",
       "15  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.317598\n",
       "16  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.314490\n",
       "17  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.316728\n",
       "18  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.322318\n",
       "19  ```\\n\\n**Example:**\\n\\n```\\n{\\n  \"response\": \"...       5.323620"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persoagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
