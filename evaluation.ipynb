{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "\n",
    "# Set the logging level to ERROR to ignore warnings\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = \"Blended Skill Talk\"                                # Synthetic-PersonaChat, Blended Skill Talk, PEC, ConvAI2, FoCus, IT-ConvAI2\n",
    "LLM_name = \"Llama3-1-8B-Instruct\"                                # Mistral-7B-Instruct, Llama3-1-8B-Instruct, Qwen2-7B-Instruct,  gpt-3.5-turbo, gpt-4-turbo, gpt-4o-mini\n",
    "COT_SETUP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (980, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>act_response</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[User 1 persona]: ['i hate talking to people.'...</td>\n",
       "      <td>I think it's because in my head, I think every...</td>\n",
       "      <td>User1: Wow, I am never shy. Do you have anxiet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[User 1 persona]: ['i have three daughters.' '...</td>\n",
       "      <td>What does your turtle eat?  Is it hard to take...</td>\n",
       "      <td>User1: My turtle ran away from me today.\\nUser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[User 1 persona]: ['i hate the taste of fish.'...</td>\n",
       "      <td>Yeah, kids grow up so quickly</td>\n",
       "      <td>User1: Our son in the Army is taking a leave t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[User 1 persona]: ['my favorite movie is good ...</td>\n",
       "      <td>Wow, you've done a marathon?  I run a bit, but...</td>\n",
       "      <td>User1: that's awesome , i like running in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[User 1 persona]: ['my hair is black.' 'i like...</td>\n",
       "      <td>I would suggest a fitness place with a rock wa...</td>\n",
       "      <td>User1: Are there different skill levels? \\nUse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  [User 1 persona]: ['i hate talking to people.'...   \n",
       "1  [User 1 persona]: ['i have three daughters.' '...   \n",
       "2  [User 1 persona]: ['i hate the taste of fish.'...   \n",
       "3  [User 1 persona]: ['my favorite movie is good ...   \n",
       "4  [User 1 persona]: ['my hair is black.' 'i like...   \n",
       "\n",
       "                                        act_response  \\\n",
       "0  I think it's because in my head, I think every...   \n",
       "1  What does your turtle eat?  Is it hard to take...   \n",
       "2                     Yeah, kids grow up so quickly    \n",
       "3  Wow, you've done a marathon?  I run a bit, but...   \n",
       "4  I would suggest a fitness place with a rock wa...   \n",
       "\n",
       "                                             context  \n",
       "0  User1: Wow, I am never shy. Do you have anxiet...  \n",
       "1  User1: My turtle ran away from me today.\\nUser...  \n",
       "2  User1: Our son in the Army is taking a leave t...  \n",
       "3  User1: that's awesome , i like running in the ...  \n",
       "4  User1: Are there different skill levels? \\nUse...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'./Prompts/{Dataset}.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Blended Skill Talk'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas        0\n",
      "act_response    0\n",
      "context         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>act_response</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i hate talking to people. i believe dragons ar...</td>\n",
       "      <td>I think it's because in my head, I think every...</td>\n",
       "      <td>User1: Wow, I am never shy. Do you have anxiet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have three daughters. my wife and i like to ...</td>\n",
       "      <td>What does your turtle eat?  Is it hard to take...</td>\n",
       "      <td>User1: My turtle ran away from me today.\\nUser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i hate the taste of fish. i like to paint.</td>\n",
       "      <td>Yeah, kids grow up so quickly</td>\n",
       "      <td>User1: Our son in the Army is taking a leave t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my favorite movie is good burger. i like canni...</td>\n",
       "      <td>Wow, you've done a marathon?  I run a bit, but...</td>\n",
       "      <td>User1: that's awesome , i like running in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my hair is black. i like rock climbing.</td>\n",
       "      <td>I would suggest a fitness place with a rock wa...</td>\n",
       "      <td>User1: Are there different skill levels? \\nUse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my dad works at the mill and my mom is a teach...</td>\n",
       "      <td>I'm sure you'll do great. In second grade, tha...</td>\n",
       "      <td>User1: This is the first time I drop my kids o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  i hate talking to people. i believe dragons ar...   \n",
       "1  i have three daughters. my wife and i like to ...   \n",
       "2         i hate the taste of fish. i like to paint.   \n",
       "3  my favorite movie is good burger. i like canni...   \n",
       "4            my hair is black. i like rock climbing.   \n",
       "5  my dad works at the mill and my mom is a teach...   \n",
       "\n",
       "                                        act_response  \\\n",
       "0  I think it's because in my head, I think every...   \n",
       "1  What does your turtle eat?  Is it hard to take...   \n",
       "2                     Yeah, kids grow up so quickly    \n",
       "3  Wow, you've done a marathon?  I run a bit, but...   \n",
       "4  I would suggest a fitness place with a rock wa...   \n",
       "5  I'm sure you'll do great. In second grade, tha...   \n",
       "\n",
       "                                             context  \n",
       "0  User1: Wow, I am never shy. Do you have anxiet...  \n",
       "1  User1: My turtle ran away from me today.\\nUser...  \n",
       "2  User1: Our son in the Army is taking a leave t...  \n",
       "3  User1: that's awesome , i like running in the ...  \n",
       "4  User1: Are there different skill levels? \\nUse...  \n",
       "5  User1: This is the first time I drop my kids o...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Only For: FoCus, IT-ConvAI2\n",
    "if Dataset == \"FoCus\" or Dataset == \"IT-ConvAI2\":\n",
    "    df['act_response'] = df['act_response'].apply(lambda x: x.split(':', 1)[1].strip() if ':' in x else x.strip())\n",
    "\n",
    "# ### Only For: Blended Skill Talk\n",
    "if Dataset == \"Blended Skill Talk\":\n",
    "    df['personas'] = df['personas'].str.replace(r'\\[User 1 persona\\]:|\\[|\\]|\"|\\'', '', regex=True).str.strip()\n",
    "\n",
    "# ### Only For: PEC\n",
    "if Dataset == \"PEC\":\n",
    "    df['personas'] = df['personas'].str.replace(r'\\[Responder persona\\]:|\\[|\\]|\"|\\'', '', regex=True).str.strip()\n",
    "\n",
    "\n",
    "print(df.isnull().sum())\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (980, 2)\n",
      "\n",
      "Missing Values:\n",
      "gen_response     42\n",
      "response_time     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.267274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ha ha, that's hilarious! I can imagine my daug...</td>\n",
       "      <td>4.268464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm sure it's bittersweet for you, but it's gr...</td>\n",
       "      <td>4.292344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've found that canning can be a great way to ...</td>\n",
       "      <td>4.274875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can start by finding a local climbing gym ...</td>\n",
       "      <td>4.259016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>I’ve used mind maps for planning my hikes and ...</td>\n",
       "      <td>2.548881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>I'm glad you're excited about medical school, ...</td>\n",
       "      <td>4.354131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>That's correct, green and red are the other pr...</td>\n",
       "      <td>4.296442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>I'm so sorry to hear that. Having a child out ...</td>\n",
       "      <td>4.327972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>I think you're being too hard on yourself. Bei...</td>\n",
       "      <td>4.323426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          gen_response  response_time\n",
       "0                                                  NaN       4.267274\n",
       "1    Ha ha, that's hilarious! I can imagine my daug...       4.268464\n",
       "2    I'm sure it's bittersweet for you, but it's gr...       4.292344\n",
       "3    I've found that canning can be a great way to ...       4.274875\n",
       "4    You can start by finding a local climbing gym ...       4.259016\n",
       "..                                                 ...            ...\n",
       "975  I’ve used mind maps for planning my hikes and ...       2.548881\n",
       "976  I'm glad you're excited about medical school, ...       4.354131\n",
       "977  That's correct, green and red are the other pr...       4.296442\n",
       "978  I'm so sorry to hear that. Having a child out ...       4.327972\n",
       "979  I think you're being too hard on yourself. Bei...       4.323426\n",
       "\n",
       "[980 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COT_ = \"-COT\" if COT_SETUP else \"\"\n",
    " \n",
    "response = pd.read_csv(f'Responses/{Dataset}/{LLM_name}{COT_}.csv')\n",
    "print(\"Shape:\", response.shape)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(response.isnull().sum())\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Response Length (in words): 89\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum number of words in each column\n",
    "max_response_length = response['gen_response'].dropna().apply(lambda x: len(x.split())).max()\n",
    "\n",
    "print(f\"Maximum Response Length (in words): {max_response_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personas          0\n",
      "act_response      0\n",
      "gen_response     42\n",
      "response_time     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personas</th>\n",
       "      <th>act_response</th>\n",
       "      <th>gen_response</th>\n",
       "      <th>response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i hate talking to people. i believe dragons ar...</td>\n",
       "      <td>I think it's because in my head, I think every...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.267274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have three daughters. my wife and i like to ...</td>\n",
       "      <td>What does your turtle eat?  Is it hard to take...</td>\n",
       "      <td>Ha ha, that's hilarious! I can imagine my daug...</td>\n",
       "      <td>4.268464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i hate the taste of fish. i like to paint.</td>\n",
       "      <td>Yeah, kids grow up so quickly</td>\n",
       "      <td>I'm sure it's bittersweet for you, but it's gr...</td>\n",
       "      <td>4.292344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my favorite movie is good burger. i like canni...</td>\n",
       "      <td>Wow, you've done a marathon?  I run a bit, but...</td>\n",
       "      <td>I've found that canning can be a great way to ...</td>\n",
       "      <td>4.274875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my hair is black. i like rock climbing.</td>\n",
       "      <td>I would suggest a fitness place with a rock wa...</td>\n",
       "      <td>You can start by finding a local climbing gym ...</td>\n",
       "      <td>4.259016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            personas  \\\n",
       "0  i hate talking to people. i believe dragons ar...   \n",
       "1  i have three daughters. my wife and i like to ...   \n",
       "2         i hate the taste of fish. i like to paint.   \n",
       "3  my favorite movie is good burger. i like canni...   \n",
       "4            my hair is black. i like rock climbing.   \n",
       "\n",
       "                                        act_response  \\\n",
       "0  I think it's because in my head, I think every...   \n",
       "1  What does your turtle eat?  Is it hard to take...   \n",
       "2                     Yeah, kids grow up so quickly    \n",
       "3  Wow, you've done a marathon?  I run a bit, but...   \n",
       "4  I would suggest a fitness place with a rock wa...   \n",
       "\n",
       "                                        gen_response  response_time  \n",
       "0                                                NaN       4.267274  \n",
       "1  Ha ha, that's hilarious! I can imagine my daug...       4.268464  \n",
       "2  I'm sure it's bittersweet for you, but it's gr...       4.292344  \n",
       "3  I've found that canning can be a great way to ...       4.274875  \n",
       "4  You can start by finding a local climbing gym ...       4.259016  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text, remove_stop_words=True):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Removing punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    if remove_stop_words:\n",
    "        tokens = [word for word in tokens if word not in stop_words]  # Removing stop words\n",
    "    return ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "# Create eval_df\n",
    "eval_df = pd.DataFrame({\n",
    "    'personas': df['personas'],\n",
    "    'act_response': df['act_response'],\n",
    "    'gen_response': response['gen_response'],\n",
    "    'response_time': response['response_time']\n",
    "})\n",
    "\n",
    "print(eval_df.isnull().sum())\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1  # device set to 0 for GPU, -1 for CPU\n",
    "# device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge\n",
    "import bert_score\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = Rouge()\n",
    "\n",
    "# Lists to store the metrics\n",
    "bleu_scores = []\n",
    "rouge_scores = []\n",
    "meteor_scores = []\n",
    "bertscore_prec = []\n",
    "bertscore_rec = []\n",
    "bertscore_f1 = []\n",
    "distinct_1 = []\n",
    "distinct_2 = []\n",
    "ue_scores = []\n",
    "c_scores = []\n",
    "consistency_scores = []\n",
    "idf_scores = []\n",
    "persona_distance_scores = []\n",
    "\n",
    "\n",
    "bert_snli_dir = \"Fine-tuning/output/bert_snli\"\n",
    "bert_snli_model = BertForSequenceClassification.from_pretrained(bert_snli_dir)\n",
    "bert_snli_tokenizer = BertTokenizer.from_pretrained(bert_snli_dir)\n",
    "\n",
    "# Initialize the NLI pipeline for UE Score\n",
    "bert_on_snli = pipeline('text-classification', model = bert_snli_model, tokenizer = bert_snli_tokenizer, device=0)\n",
    "\n",
    "bert_dnli_dir = \"Fine-tuning/output/bert_dnli\"\n",
    "bert_dnli_model = BertForSequenceClassification.from_pretrained(bert_dnli_dir)\n",
    "bert_dnli_tokenizer = BertTokenizer.from_pretrained(bert_dnli_dir)\n",
    "\n",
    "# Initialize the NLI pipeline\n",
    "bert_on_dnli = pipeline('text-classification', model = bert_dnli_model, tokenizer = bert_dnli_tokenizer, device=0)\n",
    "\n",
    "\n",
    "# Initialize the Word2Vec Model\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Initialize smoothing function\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "# Helper functions\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    reference = [str(reference).replace('\\n', ' ').split()]\n",
    "    hypothesis = str(hypothesis).replace('\\n', ' ').split()\n",
    "    return sentence_bleu(reference, hypothesis, smoothing_function=smoothing_function)\n",
    "\n",
    "def compute_rouge(reference, hypothesis):\n",
    "    scores = rouge.get_scores(str(hypothesis).replace('\\n', ' '), str(reference).replace('\\n', ' '), avg=True)\n",
    "    return scores['rouge-1']['f'], scores['rouge-2']['f'], scores['rouge-l']['f']\n",
    "\n",
    "def compute_meteor(reference, hypothesis):\n",
    "    reference = [str(reference).replace('\\n', ' ').split()]\n",
    "    hypothesis = str(hypothesis).replace('\\n', ' ').split()\n",
    "    return meteor_score(reference, hypothesis)\n",
    "\n",
    "def compute_distinct_ngrams(text, n):\n",
    "    tokens = str(text).replace('\\n', ' ').split()\n",
    "    ngrams = list(zip(*[tokens[i:] for i in range(n)]))\n",
    "    distinct_ngrams = len(set(ngrams))\n",
    "    total_ngrams = len(ngrams)\n",
    "    return distinct_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
    "\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_c_score(gen_response, persona):\n",
    "    \"\"\"\n",
    "    Calculate the C score based on the entailment results between a generated response (R)\n",
    "    and a given persona (P).\n",
    "\n",
    "    Returns:\n",
    "    int: C-score with possible values:\n",
    "         1 for entailment (positive),\n",
    "         0 for neutral,\n",
    "         -1 for contradiction (negative).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the label mapping to interpret the NLI model's output\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'negative',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'positive'\n",
    "    }\n",
    "    \n",
    "    # Check entailment between persona (P) and generated response (R)\n",
    "    result_pr = bert_on_dnli(f\"{persona} {gen_response}\")\n",
    "    label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "    # Determine C score based on entailment results\n",
    "    if label_pr == 'positive':\n",
    "        return 1\n",
    "    elif label_pr == 'neutral':\n",
    "        return 0\n",
    "    elif label_pr == 'negative':\n",
    "        return -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected label encountered: {label_pr}\")\n",
    "\n",
    "\n",
    "def calculate_consistency_score(gen_response, persona):\n",
    "    \"\"\"\n",
    "    Calculate the Consistency Score based on the binary entailment results \n",
    "    between a generated response (R) and a given persona (P).\n",
    "\n",
    "    Returns:\n",
    "    int: Consistency Score with binary values:\n",
    "         1 for entailment or neutral,\n",
    "         0 for contradiction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the label mapping for binary classification\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'negative',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'positive'\n",
    "    }\n",
    "\n",
    "    # Check entailment between persona (P) and generated response (R)\n",
    "    result_pr = bert_on_dnli(f\"{persona} {gen_response}\")\n",
    "    label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "    # Determine Consistency Score based on binary entailment results\n",
    "    if label_pr in ['positive', 'neutral']:\n",
    "        return 1\n",
    "    elif label_pr == 'negative':\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected label encountered: {label_pr}\")\n",
    "\n",
    "\n",
    "def calculate_ue_score(act_response, gen_response, persona):\n",
    "    \"\"\"\n",
    "    Calculate the UE score based on entailment between persona, actual response, and generated response.\n",
    "\n",
    "    Returns:\n",
    "    int: UE score with possible values 2, 1, or 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the label mapping to interpret the NLI model's output\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'entailment',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'contradiction'\n",
    "    }\n",
    "    \n",
    "    # Check entailment between persona (P) and generated response (R)\n",
    "    result_pr = bert_on_snli(f\"{persona} [SEP] {gen_response}\")\n",
    "    label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "    # Check entailment between actual response (Q) and generated response (R)\n",
    "    result_qr = bert_on_snli(f\"{act_response} [SEP] {gen_response}\")\n",
    "    label_qr = label_mapping.get(result_qr[0]['label'], 'unknown')\n",
    "\n",
    "    # Determine UE score based on entailment results\n",
    "    if label_pr == 'entailment' and label_qr == 'entailment':\n",
    "        return 2\n",
    "    elif label_pr == 'entailment':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def calculate_idf_weighted_overlap(persona, response):\n",
    "    # Fit TF-IDF on both texts and calculate cosine similarity\n",
    "\n",
    "    processed_persona = preprocess_text(persona)\n",
    "    processed_response = preprocess_text(response)\n",
    "    persona_new = str(processed_persona) if not isinstance(processed_persona, str) else processed_persona\n",
    "    response_new = str(processed_response) if not isinstance(processed_response, str) else processed_response\n",
    "    texts = [persona_new, response_new]\n",
    "\n",
    "    # texts = [persona, response]\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return cosine_sim[0][0]\n",
    "\n",
    "\n",
    "def compute_persona_distance(persona, response, model, stop_words):\n",
    "    # Tokenize and filter stopwords\n",
    "    persona_tokens = [word for word in persona.lower().split() if word not in stop_words]\n",
    "    response_tokens = [word for word in response.lower().split() if word not in stop_words]\n",
    "    \n",
    "    # Get word vectors\n",
    "    persona_vecs = [model[word] for word in persona_tokens if word in model]\n",
    "    response_vecs = [model[word] for word in response_tokens if word in model]\n",
    "    \n",
    "    # If no vectors found, return zero similarity\n",
    "    if not persona_vecs or not response_vecs:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute average vectors\n",
    "    persona_avg_vec = np.mean(persona_vecs, axis=0)\n",
    "    response_avg_vec = np.mean(response_vecs, axis=0)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    return cosine_similarity([persona_avg_vec], [response_avg_vec])[0][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the logging level to ERROR to suppress warnings about training\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Default worst-case values\n",
    "worst_bleu = 0.0\n",
    "worst_rouge = (0.0, 0.0, 0.0)\n",
    "worst_meteor = 0.0\n",
    "worst_bertscore = (0.0, 0.0, 0.0)\n",
    "worst_distinct = 0.0\n",
    "worst_c_score = -1.0\n",
    "worst_consistency_score = 0.0\n",
    "worst_idf_score = 0.0\n",
    "worst_ue_score = 0.0\n",
    "worst_persona_distance_score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter for invalid gen_response\n",
    "invalid_gen_res_count = 0\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n",
    "    personas = row['personas']\n",
    "    act_response = row['act_response']\n",
    "    gen_response = row['gen_response']\n",
    "\n",
    "    # Check for NaN or None in gen_response\n",
    "    if pd.isna(gen_response):\n",
    "        invalid_gen_res_count += 1\n",
    "        \n",
    "        bleu_scores.append(worst_bleu)\n",
    "        rouge_scores.append(worst_rouge)\n",
    "        meteor_scores.append(worst_meteor)\n",
    "        bertscore_prec.append(worst_bertscore[0])\n",
    "        bertscore_rec.append(worst_bertscore[1])\n",
    "        bertscore_f1.append(worst_bertscore[2])\n",
    "        distinct_1.append(worst_distinct)\n",
    "        distinct_2.append(worst_distinct)\n",
    "        c_scores.append(worst_c_score)\n",
    "        consistency_scores.append(worst_consistency_score)\n",
    "        idf_scores.append(worst_idf_score)\n",
    "        persona_distance_scores.append(worst_persona_distance_score)\n",
    "        ue_scores.append(worst_ue_score)\n",
    "\n",
    "        continue\n",
    "\n",
    "    bleu = compute_bleu(act_response, gen_response)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    rouge_1, rouge_2, rouge_l = compute_rouge(act_response, gen_response)\n",
    "    rouge_scores.append((rouge_1, rouge_2, rouge_l))\n",
    "    \n",
    "    meteor = compute_meteor(act_response, gen_response)\n",
    "    meteor_scores.append(meteor)\n",
    "    \n",
    "    P, R, F1 = bert_score.score([gen_response], [act_response], lang=\"en\", verbose=False)\n",
    "    bertscore_prec.append(P.mean().item())\n",
    "    bertscore_rec.append(R.mean().item())\n",
    "    bertscore_f1.append(F1.mean().item())\n",
    "    \n",
    "    distinct_1.append(compute_distinct_ngrams(gen_response, 1))\n",
    "    \n",
    "    distinct_2.append(compute_distinct_ngrams(gen_response, 2))\n",
    "    \n",
    "    c_scores.append(calculate_c_score(personas, gen_response))\n",
    "    \n",
    "    consistency_scores.append(calculate_consistency_score(personas, gen_response))\n",
    "    \n",
    "    ue_scores.append(calculate_ue_score(act_response, gen_response, personas))\n",
    "\n",
    "    idf_scores.append(calculate_idf_weighted_overlap(personas, gen_response))\n",
    "    \n",
    "    persona_distance = compute_persona_distance(personas, gen_response, word2vec_model, stop_words)\n",
    "    persona_distance_scores.append(persona_distance)\n",
    "\n",
    "\n",
    "# Compile metrics into DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'BLEU': bleu_scores,\n",
    "    'R1': [score[0] for score in rouge_scores],\n",
    "    'R2': [score[1] for score in rouge_scores],\n",
    "    'RL': [score[2] for score in rouge_scores],\n",
    "    'METEOR': meteor_scores,\n",
    "    'BERTScore_Prec': bertscore_prec,\n",
    "    'BERTScore_Rec': bertscore_rec,\n",
    "    'BERTScore_F1': bertscore_f1,\n",
    "    'Dist1': distinct_1,\n",
    "    'Dist2': distinct_2,\n",
    "    'C Score': c_scores,\n",
    "    'P Consistency Score': consistency_scores,\n",
    "    'IDF Overlap': idf_scores,\n",
    "    'UE Score': ue_scores,\n",
    "    'Persona Distance': persona_distance_scores\n",
    "})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'response_time' column to 'metrics_df'\n",
    "metrics_df['response_time'] = eval_df['response_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean (average) and standard deviation, rounded to 2 decimal places\n",
    "avg_values = metrics_df.mean().round(2)\n",
    "std_values = metrics_df.std(ddof=0).round(2)  # Use ddof=0 for population standard deviation\n",
    "\n",
    "# Combine the average and standard deviation into the format \"avg ± std\"\n",
    "combined_values = avg_values.astype(str) + \" ± \" + std_values.astype(str)\n",
    "\n",
    "# Insert the LLM name at the beginning of the combined values\n",
    "combined_values = combined_values.tolist()\n",
    "combined_values.insert(0, LLM_name)\n",
    "\n",
    "# Create a DataFrame for the combined average ± std row\n",
    "result_df = pd.DataFrame([combined_values], columns=['Model'] + metrics_df.columns.tolist())\n",
    "\n",
    "# Add the ratio of invalid gen_response\n",
    "invalid_gen_res_ratio = invalid_gen_res_count / len(eval_df)\n",
    "result_df['Failure Ratio'] = f\"{round(invalid_gen_res_ratio, 3)} ± 0.00\"  # No std for Failure Ratio\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing Excel file and update or append the average row\n",
    "output_path = f'./Evaluations/{Dataset}{COT_}-results.xlsx'\n",
    "\n",
    "try:\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_excel(output_path)\n",
    "    # Check if the model name already exists\n",
    "    if LLM_name in existing_df['Model'].values:\n",
    "        # Update the row with the same model name\n",
    "        existing_df.loc[existing_df['Model'] == LLM_name, :] = result_df.values\n",
    "    else:\n",
    "        # Append the new data\n",
    "        existing_df = pd.concat([existing_df, result_df], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create a new DataFrame\n",
    "    existing_df = result_df\n",
    "\n",
    "# Save the updated DataFrame to an Excel file\n",
    "existing_df.to_excel(output_path, index=False)\n",
    "\n",
    "existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>P Consistency Score</th>\n",
       "      <th>C Score</th>\n",
       "      <th>UE Score</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "      <th>RL</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>BERTScore_Prec</th>\n",
       "      <th>...</th>\n",
       "      <th>BERTScore_F1</th>\n",
       "      <th>Dist1</th>\n",
       "      <th>Dist2</th>\n",
       "      <th>C-Score</th>\n",
       "      <th>P-Score</th>\n",
       "      <th>IDF Overlap</th>\n",
       "      <th>Coh-Con Score</th>\n",
       "      <th>Persona Distance</th>\n",
       "      <th>response_time</th>\n",
       "      <th>Failure Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mistral-7B-Instruct</td>\n",
       "      <td>0.84 ± 0.36\\t</td>\n",
       "      <td>0.33 ± 0.73\\t</td>\n",
       "      <td>0.46 ± 0.72\\t</td>\n",
       "      <td>0.01 ± 0.02</td>\n",
       "      <td>0.12 ± 0.1</td>\n",
       "      <td>0.02 ± 0.05</td>\n",
       "      <td>0.11 ± 0.09</td>\n",
       "      <td>0.14 ± 0.11</td>\n",
       "      <td>0.81 ± 0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.81 ± 0.18</td>\n",
       "      <td>0.86 ± 0.2</td>\n",
       "      <td>0.95 ± 0.21</td>\n",
       "      <td>0.08 ± 0.49</td>\n",
       "      <td>0.91 ± 0.28</td>\n",
       "      <td>0.14 ± 0.12</td>\n",
       "      <td>0.19 ± 0.44</td>\n",
       "      <td>0.62 ± 0.2</td>\n",
       "      <td>3.11 ± 1.11</td>\n",
       "      <td>0.047 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama3-1-8B-Instruct</td>\n",
       "      <td>0.76 ± 0.43</td>\n",
       "      <td>0.03 ± 0.72</td>\n",
       "      <td>0.23 ± 0.58\\t</td>\n",
       "      <td>0.01 ± 0.02</td>\n",
       "      <td>0.11 ± 0.09</td>\n",
       "      <td>0.02 ± 0.05</td>\n",
       "      <td>0.1 ± 0.09</td>\n",
       "      <td>0.13 ± 0.1</td>\n",
       "      <td>0.76 ± 0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.76 ± 0.25</td>\n",
       "      <td>0.78 ± 0.27</td>\n",
       "      <td>0.89 ± 0.3</td>\n",
       "      <td>-0.11 ± 0.38</td>\n",
       "      <td>0.86 ± 0.34</td>\n",
       "      <td>0.08 ± 0.08</td>\n",
       "      <td>0.02 ± 0.17</td>\n",
       "      <td>0.55 ± 0.22</td>\n",
       "      <td>4.15 ± 0.53</td>\n",
       "      <td>0.098 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2-7B-Instruct</td>\n",
       "      <td>0.84 ± 0.36\\t</td>\n",
       "      <td>0.21 ± 0.7</td>\n",
       "      <td>0.42 ± 0.73</td>\n",
       "      <td>0.01 ± 0.01</td>\n",
       "      <td>0.1 ± 0.08</td>\n",
       "      <td>0.02 ± 0.04</td>\n",
       "      <td>0.1 ± 0.08</td>\n",
       "      <td>0.14 ± 0.1</td>\n",
       "      <td>0.83 ± 0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83 ± 0.12</td>\n",
       "      <td>0.9 ± 0.14</td>\n",
       "      <td>0.98 ± 0.14</td>\n",
       "      <td>0.01 ± 0.35</td>\n",
       "      <td>0.94 ± 0.23</td>\n",
       "      <td>0.1 ± 0.11</td>\n",
       "      <td>0.07 ± 0.28</td>\n",
       "      <td>0.61 ± 0.17</td>\n",
       "      <td>1.97 ± 0.72</td>\n",
       "      <td>0.019 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0.85 ± 0.36\\t</td>\n",
       "      <td>0.23 ± 0.69</td>\n",
       "      <td>0.49 ± 0.76</td>\n",
       "      <td>0.01 ± 0.02</td>\n",
       "      <td>0.12 ± 0.09</td>\n",
       "      <td>0.02 ± 0.05</td>\n",
       "      <td>0.11 ± 0.08</td>\n",
       "      <td>0.15 ± 0.11</td>\n",
       "      <td>0.85 ± 0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85 ± 0.02</td>\n",
       "      <td>0.91 ± 0.06</td>\n",
       "      <td>1.0 ± 0.01</td>\n",
       "      <td>0.04 ± 0.34</td>\n",
       "      <td>0.96 ± 0.2</td>\n",
       "      <td>0.13 ± 0.13</td>\n",
       "      <td>0.08 ± 0.29</td>\n",
       "      <td>0.63 ± 0.15</td>\n",
       "      <td>1.18 ± 0.57</td>\n",
       "      <td>0.0 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4-turbo</td>\n",
       "      <td>0.85 ± 0.36\\t</td>\n",
       "      <td>0.08 ± 0.62</td>\n",
       "      <td>0.43 ± 0.78</td>\n",
       "      <td>0.01 ± 0.02</td>\n",
       "      <td>0.12 ± 0.09</td>\n",
       "      <td>0.02 ± 0.05</td>\n",
       "      <td>0.11 ± 0.08</td>\n",
       "      <td>0.15 ± 0.11</td>\n",
       "      <td>0.85 ± 0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.85 ± 0.02</td>\n",
       "      <td>0.93 ± 0.05</td>\n",
       "      <td>1.0 ± 0.01</td>\n",
       "      <td>-0.03 ± 0.27</td>\n",
       "      <td>0.95 ± 0.22</td>\n",
       "      <td>0.09 ± 0.08</td>\n",
       "      <td>0.02 ± 0.17</td>\n",
       "      <td>0.6 ± 0.13</td>\n",
       "      <td>2.21 ± 0.71</td>\n",
       "      <td>0.0 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>0.85 ± 0.36</td>\n",
       "      <td>0.1 ± 0.63</td>\n",
       "      <td>0.4 ± 0.75</td>\n",
       "      <td>0.01 ± 0.01</td>\n",
       "      <td>0.11 ± 0.07</td>\n",
       "      <td>0.02 ± 0.04</td>\n",
       "      <td>0.1 ± 0.07</td>\n",
       "      <td>0.16 ± 0.1</td>\n",
       "      <td>0.84 ± 0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84 ± 0.05</td>\n",
       "      <td>0.89 ± 0.07</td>\n",
       "      <td>0.99 ± 0.05</td>\n",
       "      <td>-0.01 ± 0.18</td>\n",
       "      <td>0.98 ± 0.15</td>\n",
       "      <td>0.1 ± 0.08</td>\n",
       "      <td>0.01 ± 0.11</td>\n",
       "      <td>0.63 ± 0.12</td>\n",
       "      <td>0.99 ± 0.37</td>\n",
       "      <td>0.003 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model P Consistency Score        C Score       UE Score  \\\n",
       "0   Mistral-7B-Instruct       0.84 ± 0.36\\t  0.33 ± 0.73\\t  0.46 ± 0.72\\t   \n",
       "1  Llama3-1-8B-Instruct         0.76 ± 0.43    0.03 ± 0.72  0.23 ± 0.58\\t   \n",
       "2     Qwen2-7B-Instruct       0.84 ± 0.36\\t     0.21 ± 0.7    0.42 ± 0.73   \n",
       "3         gpt-3.5-turbo       0.85 ± 0.36\\t    0.23 ± 0.69    0.49 ± 0.76   \n",
       "4           gpt-4-turbo       0.85 ± 0.36\\t    0.08 ± 0.62    0.43 ± 0.78   \n",
       "5           gpt-4o-mini         0.85 ± 0.36     0.1 ± 0.63     0.4 ± 0.75   \n",
       "\n",
       "          BLEU           R1           R2           RL       METEOR  \\\n",
       "0  0.01 ± 0.02   0.12 ± 0.1  0.02 ± 0.05  0.11 ± 0.09  0.14 ± 0.11   \n",
       "1  0.01 ± 0.02  0.11 ± 0.09  0.02 ± 0.05   0.1 ± 0.09   0.13 ± 0.1   \n",
       "2  0.01 ± 0.01   0.1 ± 0.08  0.02 ± 0.04   0.1 ± 0.08   0.14 ± 0.1   \n",
       "3  0.01 ± 0.02  0.12 ± 0.09  0.02 ± 0.05  0.11 ± 0.08  0.15 ± 0.11   \n",
       "4  0.01 ± 0.02  0.12 ± 0.09  0.02 ± 0.05  0.11 ± 0.08  0.15 ± 0.11   \n",
       "5  0.01 ± 0.01  0.11 ± 0.07  0.02 ± 0.04   0.1 ± 0.07   0.16 ± 0.1   \n",
       "\n",
       "  BERTScore_Prec  ... BERTScore_F1        Dist1        Dist2       C-Score  \\\n",
       "0    0.81 ± 0.18  ...  0.81 ± 0.18   0.86 ± 0.2  0.95 ± 0.21   0.08 ± 0.49   \n",
       "1    0.76 ± 0.25  ...  0.76 ± 0.25  0.78 ± 0.27   0.89 ± 0.3  -0.11 ± 0.38   \n",
       "2    0.83 ± 0.12  ...  0.83 ± 0.12   0.9 ± 0.14  0.98 ± 0.14   0.01 ± 0.35   \n",
       "3    0.85 ± 0.02  ...  0.85 ± 0.02  0.91 ± 0.06   1.0 ± 0.01   0.04 ± 0.34   \n",
       "4    0.85 ± 0.02  ...  0.85 ± 0.02  0.93 ± 0.05   1.0 ± 0.01  -0.03 ± 0.27   \n",
       "5    0.84 ± 0.05  ...  0.84 ± 0.05  0.89 ± 0.07  0.99 ± 0.05  -0.01 ± 0.18   \n",
       "\n",
       "       P-Score  IDF Overlap Coh-Con Score Persona Distance response_time  \\\n",
       "0  0.91 ± 0.28  0.14 ± 0.12   0.19 ± 0.44       0.62 ± 0.2   3.11 ± 1.11   \n",
       "1  0.86 ± 0.34  0.08 ± 0.08   0.02 ± 0.17      0.55 ± 0.22   4.15 ± 0.53   \n",
       "2  0.94 ± 0.23   0.1 ± 0.11   0.07 ± 0.28      0.61 ± 0.17   1.97 ± 0.72   \n",
       "3   0.96 ± 0.2  0.13 ± 0.13   0.08 ± 0.29      0.63 ± 0.15   1.18 ± 0.57   \n",
       "4  0.95 ± 0.22  0.09 ± 0.08   0.02 ± 0.17       0.6 ± 0.13   2.21 ± 0.71   \n",
       "5  0.98 ± 0.15   0.1 ± 0.08   0.01 ± 0.11      0.63 ± 0.12   0.99 ± 0.37   \n",
       "\n",
       "  Failure Ratio  \n",
       "0  0.047 ± 0.00  \n",
       "1  0.098 ± 0.00  \n",
       "2  0.019 ± 0.00  \n",
       "3    0.0 ± 0.00  \n",
       "4    0.0 ± 0.00  \n",
       "5  0.003 ± 0.00  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pd.read_excel(f'./Evaluations/{Dataset}{COT_}-results.xlsx')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/salehafzoon/Desktop/Perso-LLM-Benchmarking/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import gensim\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1  # device set to 0 for GPU, -1 for CPU\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"./GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "nli_model = pipeline('text-classification', model='facebook/bart-large-mnli', device=device)\n",
    "\n",
    "persona_text = \"I am a software engineer. I love coding in Python. I also enjoy hiking during weekends.\"\n",
    "response_text = \"Coding in Python is one of my favorite activities. On weekends, I often go hiking.\"\n",
    "gen_response_text = \"On weekends, I often go hiking, and Python coding is something I really enjoy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00336669  1.         -0.01056319  0.13576175  0.11221063  0.19117984\n",
      "   0.09335288  0.1638258   1.          0.2615446 ]]\n",
      "[[ 0.18068513  0.25550863 -0.03758506  0.21812178  0.02882948  0.05052864\n",
      "   0.34898528  0.13838342  0.2555086   0.21406093]]\n",
      "[[0.00118619 0.09245683 0.03459306 0.02209745 0.14510264 0.51879144\n",
      "  0.06418717 0.0685657  0.09245681 0.0643034 ]]\n",
      "[[-0.00336669  1.         -0.01056319  0.13576175  0.11221063  0.19117984\n",
      "   0.09335288  0.1638258   1.          0.2615446 ]]\n",
      "[[0.04461894 0.16269058 0.1994664  0.20273286 0.06281446 0.03281252\n",
      "  0.06216637 0.31501308 0.16269056 0.41482794]]\n",
      "[[-2.1985812e-02  1.9117984e-01  1.2112212e-01  2.7989851e-02\n",
      "   1.6500372e-01  1.0000000e+00 -3.7418772e-04  1.0601519e-01\n",
      "   1.9117984e-01  6.1645295e-02]]\n",
      "[[0.3958767  0.01996717 0.14580318 0.15322813 0.0025511  0.02053415\n",
      "  0.243254   0.08290297 0.01996717 0.18737666]]\n",
      "[[-0.00336669  1.         -0.01056319  0.13576175  0.11221063  0.19117984\n",
      "   0.09335288  0.1638258   1.          0.2615446 ]]\n",
      "[[ 0.2104019  -0.04289635  0.3481123   0.0782875   0.03665383  0.00811716\n",
      "   0.147345    0.22326273 -0.04289632  0.28244206]]\n",
      "[[ 0.10209026  0.08480105  0.10774407  0.30473733 -0.03164742  0.00379885\n",
      "   0.14880571  0.19968794  0.08480105  0.28791246]]\n",
      "[[0.08312806 0.00245738 0.04830759 0.14001153 0.10175038 0.03978709\n",
      "  0.03277263 0.09682474 0.00245735 0.06542248]]\n",
      "[[ 0.30746818 -0.04894466  0.19001004  0.09381618 -0.03622447 -0.0037804\n",
      "   0.03164372  0.0304765  -0.04894465  0.12457976]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5649009"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_persona_distance(persona, response, word2vec):\n",
    "    \n",
    "    # persona and response are already pre-processed and stopwords are removed\n",
    "    \n",
    "    persona_tokens = [word for word in persona.lower().split()]\n",
    "    response_tokens = [word for word in response.lower().split()]\n",
    "    \n",
    "    \n",
    "    # Get embeddings for tokens if they exist in the word2vec model\n",
    "    persona_embeddings = [word2vec[word] for word in persona_tokens if word in word2vec]\n",
    "    response_embeddings = [word2vec[word] for word in response_tokens if word in word2vec]\n",
    "    \n",
    "    # Calculate similarity matrices M_i for each persona keyword embedding p_i\n",
    "    similarity_matrices = []\n",
    "    for p_i in persona_embeddings:\n",
    "        similarity_matrix = cosine_similarity([p_i], response_embeddings)\n",
    "        print(similarity_matrix)\n",
    "        similarity_matrices.append(np.max(similarity_matrix))\n",
    "    \n",
    "    # Calculate the P.Distance\n",
    "    p_distance = np.mean(similarity_matrices)\n",
    "    \n",
    "    return p_distance\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "calculate_persona_distance(persona_text, gen_response_text, word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coh_con_score(act_response, gen_response, persona):\n",
    "    # Check entailment between full Persona (P) and Generated Response (gen_response)\n",
    "    nli_result_pr = nli_model(f\"{persona} [SEP] {gen_response}\")\n",
    "    label_pr = nli_result_pr[0]['label'].lower()\n",
    "    print(f\"Persona Premise: {persona}\")\n",
    "    print(f\"Generated Response Hypothesis: {gen_response}\")\n",
    "    print(f\"NLI Result for (P, gen_response): {nli_result_pr}\")\n",
    "    is_persona_entails_response = label_pr == 'entailment'\n",
    "\n",
    "    # Check entailment between full Actual Response (act_response) and Generated Response (gen_response)\n",
    "    nli_result_qr = nli_model(f\"{act_response} [SEP] {gen_response}\")\n",
    "    label_qr = nli_result_qr[0]['label'].lower()\n",
    "    print(f\"Actual Response Premise: {act_response}\")\n",
    "    print(f\"Generated Response Hypothesis: {gen_response}\")\n",
    "    print(f\"NLI Result for (act_response, gen_response): {nli_result_qr}\")\n",
    "    is_act_entails_response = label_qr == 'entailment'\n",
    "\n",
    "    # Assign the Coh-Con.Score based on the entailment results\n",
    "    if is_persona_entails_response and is_act_entails_response:\n",
    "        score = 2\n",
    "    elif is_persona_entails_response:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona Premise: I am a software engineer. I love coding in Python. I also enjoy hiking during weekends.\n",
      "Generated Response Hypothesis: On weekends, I often go hiking, and Python coding is something I really enjoy.\n",
      "NLI Result for (P, gen_response): [{'label': 'entailment', 'score': 0.9853878617286682}]\n",
      "Actual Response Premise: Coding in Python is one of my favorite activities. On weekends, I often go hiking.\n",
      "Generated Response Hypothesis: On weekends, I often go hiking, and Python coding is something I really enjoy.\n",
      "NLI Result for (act_response, gen_response): [{'label': 'entailment', 'score': 0.9843922853469849}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_coh_con_score(response_text, gen_response_text, persona_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the fine-tuned BERT on SNLI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels of the SNLI dataset:\n",
    "\n",
    "- 0: entailment\n",
    "- 1: neutral\n",
    "- 2: contradiction\n",
    "\n",
    "\n",
    "UE-score:\n",
    "\n",
    "- 2: R is aligned with P and Q\n",
    "- 1: R is alinged with P\n",
    "- 0: no alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the model and tokenizer (assuming the model is already fine-tuned on SNLI)\n",
    "model_dir = \"Fine-tuning/output/bert_snli\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the NLI pipeline\n",
    "bert_on_snli = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def calculate_ue_score(act_response, gen_response, persona):\n",
    "    \"\"\"\n",
    "    Calculate the UE score based on entailment between persona, actual response, and generated response.\n",
    "\n",
    "    Returns:\n",
    "    int: UE score with possible values 2, 1, or 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the label mapping to interpret the NLI model's output\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'entailment',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'contradiction'\n",
    "    }\n",
    "    \n",
    "    # Check entailment between persona (P) and generated response (R)\n",
    "    result_pr = bert_on_snli(f\"{persona} [SEP] {gen_response}\")\n",
    "    label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "    # Check entailment between actual response (Q) and generated response (R)\n",
    "    result_qr = bert_on_snli(f\"{act_response} [SEP] {gen_response}\")\n",
    "    label_qr = label_mapping.get(result_qr[0]['label'], 'unknown')\n",
    "\n",
    "    # Determine UE score based on entailment results\n",
    "    if label_pr == 'entailment' and label_qr == 'entailment':\n",
    "        return 2\n",
    "    elif label_pr == 'entailment':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual response\n",
    "act_response = \"Coding in Python is one of my favorite activities. On weekends, I often go hiking.\"\n",
    "\n",
    "# Generated response\n",
    "gen_response_text = \"On weekends, I often go hiking, and Python coding is something I really enjoy.\"\n",
    "\n",
    "persona_text = \"I am a software engineer. I love coding in Python. I also enjoy hiking during weekends.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ue_score(act_response, gen_response_text, persona_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the fine-tuned BERT on DNLI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels of the DNLI dataset:\n",
    "\n",
    "- 0: negative\n",
    "- 1: neutral\n",
    "- 2: positive\n",
    "\n",
    "C-score:\n",
    "\n",
    "- 1: R,P entailment\n",
    "- 0: R,P neutral\n",
    "- -1: R,P contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the model and tokenizer (assuming the model is already fine-tuned on DNLI)\n",
    "model_dir = \"Fine-tuning/output/bert_dnli\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Initialize the NLI pipeline\n",
    "bert_on_dnli = pipeline('text-classification', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "def calculate_c_score(gen_response, persona):\n",
    "    \"\"\"\n",
    "    Calculate the C score based on the entailment results between a generated response (R)\n",
    "    and a given persona (P).\n",
    "\n",
    "    Returns:\n",
    "    int: C-score with possible values:\n",
    "         1 for entailment (positive),\n",
    "         0 for neutral,\n",
    "         -1 for contradiction (negative).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the label mapping to interpret the NLI model's output\n",
    "    label_mapping = {\n",
    "        'LABEL_0': 'negative',\n",
    "        'LABEL_1': 'neutral',\n",
    "        'LABEL_2': 'positive'\n",
    "    }\n",
    "    \n",
    "    # Check entailment between persona (P) and generated response (R)\n",
    "    result_pr = bert_on_dnli(f\"{persona} {gen_response}\")\n",
    "    label_pr = label_mapping.get(result_pr[0]['label'], 'unknown')\n",
    "\n",
    "    # Determine C score based on entailment results\n",
    "    if label_pr == 'positive':\n",
    "        return 1\n",
    "    elif label_pr == 'neutral':\n",
    "        return 0\n",
    "    elif label_pr == 'negative':\n",
    "        return -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected label encountered: {label_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated response\n",
    "gen_response_text = \"On weekends, I often go hiking, and Python coding is something I really enjoy.\"\n",
    "\n",
    "persona_text = \"I am a software engineer. I love coding in Python. I also enjoy hiking during weekends.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_c_score(gen_response_text, persona_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
